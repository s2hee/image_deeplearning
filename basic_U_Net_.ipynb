{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(5,5)\n",
    " \n",
    "    def forward(self,x):\n",
    "        net = self.lin1(x)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4120, -0.0506,  0.2211,  0.2685, -0.3179], device='mps:0',\n",
      "       grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# MPS 장치에 바로 tensor를 생성합니다.\n",
    "x = torch.ones(5, device=device)\n",
    " \n",
    "# GPU 상에서 연산 진행\n",
    "y = x * 2\n",
    " \n",
    "# 또는, 다른 장치와 마찬가지로 MPS로 이동할 수도 있습니다.\n",
    "model = Net()# 어떤 모델의 객체를 생성한 뒤,\n",
    "model.to(device) # MPS 장치로 이동합니다.\n",
    " \n",
    "# 이제 모델과 텐서를 호출하면 GPU에서 연산이 이뤄집니다.\n",
    "pred = model(x)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:1.13.0\n",
      "MPS 장치를 지원하도록 build 되었는지: True\n",
      "MPS 장치가 사용 가능한지: True\n",
      "macOS-10.16-x86_64-i386-64bit\n"
     ]
    }
   ],
   "source": [
    "print (f\"PyTorch version:{torch.__version__}\") # 1.12.1 이상\n",
    "print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\") # True 여야 합니다.\n",
    "print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\") # True 여야 합니다.\n",
    "!python -c 'import platform;print(platform.platform())'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading annotations.json\n",
    "TRAIN_ANNOTATIONS_PATH = \"/Users/s2hee/Desktop/deep_learning/public_training_set_release_2.1/training_annotations.json\"\n",
    "TRAIN_IMAGE_DIRECTIORY = \"/Users/s2hee/Desktop/deep_learning/public_training_set_release_2.1/images/\"\n",
    "\n",
    "VAL_ANNOTATIONS_PATH = \"/Users/s2hee/Desktop/deep_learning/public_validation_set_release_2.1/vdset_annotations.json\"\n",
    "VAL_IMAGE_DIRECTIORY = \"/Users/s2hee/Desktop/deep_learning/public_validation_set_release_2.1/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.38s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# For reading annotations file\n",
    "import pycocotools\n",
    "from pycocotools.coco import COCO\n",
    "train_coco = COCO(TRAIN_ANNOTATIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the annotation files\n",
    "import json\n",
    "with open(TRAIN_ANNOTATIONS_PATH) as f:\n",
    "  train_annotations_data = json.load(f)\n",
    "\n",
    "with open(VAL_ANNOTATIONS_PATH) as f:\n",
    "  val_annotations_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 184123,\n",
       " 'image_id': 131072,\n",
       " 'category_id': 101246,\n",
       " 'segmentation': [[169.0,\n",
       "   379.5,\n",
       "   130.0,\n",
       "   374.5,\n",
       "   112.0,\n",
       "   363.5,\n",
       "   94.5,\n",
       "   340.0,\n",
       "   61.5,\n",
       "   213.0,\n",
       "   61.5,\n",
       "   188.0,\n",
       "   70.5,\n",
       "   168.0,\n",
       "   87.0,\n",
       "   152.5,\n",
       "   103.0,\n",
       "   143.5,\n",
       "   123.0,\n",
       "   139.5,\n",
       "   185.0,\n",
       "   118.5,\n",
       "   226.0,\n",
       "   90.5,\n",
       "   249.0,\n",
       "   87.5,\n",
       "   309.0,\n",
       "   88.5,\n",
       "   339.0,\n",
       "   110.5,\n",
       "   350.5,\n",
       "   125.00000000000001,\n",
       "   354.5,\n",
       "   155.0,\n",
       "   382.5,\n",
       "   231.0,\n",
       "   383.5,\n",
       "   277.0,\n",
       "   360.0,\n",
       "   303.5,\n",
       "   327.0,\n",
       "   331.5,\n",
       "   308.0,\n",
       "   343.5,\n",
       "   216.0,\n",
       "   373.5]],\n",
       " 'area': 71393.0,\n",
       " 'bbox': [61.5, 61.5, 318.0, 322.0],\n",
       " 'iscrowd': 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations_data['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 233459,\n",
       " 'image_id': 149022,\n",
       " 'category_id': 101182,\n",
       " 'segmentation': [[214.0,\n",
       "   152.5,\n",
       "   175.0,\n",
       "   144.5,\n",
       "   156.5,\n",
       "   133.0,\n",
       "   153.5,\n",
       "   119.0,\n",
       "   159.5,\n",
       "   99.0,\n",
       "   168.0,\n",
       "   89.5,\n",
       "   191.0,\n",
       "   77.5,\n",
       "   231.99999999999997,\n",
       "   73.5,\n",
       "   258.0,\n",
       "   80.5,\n",
       "   279.5,\n",
       "   95.0,\n",
       "   285.5,\n",
       "   104.0,\n",
       "   288.5,\n",
       "   122.0,\n",
       "   278.0,\n",
       "   137.5,\n",
       "   250.00000000000003,\n",
       "   148.5]],\n",
       " 'area': 8225.0,\n",
       " 'bbox': [77.5, 153.5, 136.5, 135.0],\n",
       " 'iscrowd': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_annotations_data['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Categories\n",
      "- Beetroot, steamed, without addition of salt\n",
      "- bread_wholemeal\n",
      "- jam\n",
      "- water\n",
      "- bread\n",
      "- banana\n",
      "- soft_cheese\n",
      "- ham_raw\n",
      "- hard_cheese\n",
      "- cottage_cheese\n",
      "- coffee\n",
      "- fruit_mixed\n",
      "- pancake\n",
      "- tea\n",
      "- salmon_smoked\n",
      "- avocado\n",
      "- spring_onion_scallion\n",
      "- ristretto_with_caffeine\n",
      "- ham_n_s\n",
      "- egg\n",
      "- bacon\n",
      "- chips_french_fries\n",
      "- juice_apple\n",
      "- chicken\n",
      "- tomato\n",
      "- broccoli\n",
      "- shrimp_prawn\n",
      "- carrot\n",
      "- chickpeas\n",
      "- french_salad_dressing\n",
      "- pasta_hornli_ch\n",
      "- sauce_cream\n",
      "- pasta_n_s\n",
      "- tomato_sauce\n",
      "- cheese_n_s\n",
      "- pear\n",
      "- cashew_nut\n",
      "- almonds\n",
      "- lentil_n_s\n",
      "- mixed_vegetables\n",
      "- peanut_butter\n",
      "- apple\n",
      "- blueberries\n",
      "- cucumber\n",
      "- yogurt\n",
      "- butter\n",
      "- mayonnaise\n",
      "- soup\n",
      "- wine_red\n",
      "- wine_white\n",
      "- green_bean_steamed_without_addition_of_salt\n",
      "- sausage\n",
      "- pizza_margherita_baked\n",
      "- salami_ch\n",
      "- mushroom\n",
      "- tart_n_s\n",
      "- rice\n",
      "- white_coffee\n",
      "- sunflower_seeds\n",
      "- bell_pepper_red_raw\n",
      "- zucchini\n",
      "- asparagus\n",
      "- tartar_sauce\n",
      "- lye_pretzel_soft\n",
      "- cucumber_pickled_ch\n",
      "- curry_vegetarian\n",
      "- soup_of_lentils_dahl_dhal\n",
      "- salmon\n",
      "- salt_cake_ch_vegetables_filled\n",
      "- orange\n",
      "- pasta_noodles\n",
      "- cream_double_cream_heavy_cream_45\n",
      "- cake_chocolate\n",
      "- pasta_spaghetti\n",
      "- black_olives\n",
      "- parmesan\n",
      "- spaetzle\n",
      "- salad_lambs_ear\n",
      "- salad_leaf_salad_green\n",
      "- potato\n",
      "- white_cabbage\n",
      "- halloumi\n",
      "- beetroot_raw\n",
      "- bread_grain\n",
      "- applesauce\n",
      "- cheese_for_raclette_ch\n",
      "- bread_white\n",
      "- curds_natural\n",
      "- quiche\n",
      "- beef_n_s\n",
      "- taboule_prepared_with_couscous\n",
      "- aubergine_eggplant\n",
      "- mozzarella\n",
      "- pasta_penne\n",
      "- lasagne_vegetable_prepared\n",
      "- mandarine\n",
      "- kiwi\n",
      "- french_beans\n",
      "- spring_roll_fried\n",
      "- caprese_salad_tomato_mozzarella\n",
      "- leaf_spinach\n",
      "- roll_of_half_white_or_white_flour_with_large_void\n",
      "- omelette_with_flour_thick_crepe_plain\n",
      "- tuna\n",
      "- dark_chocolate\n",
      "- sauce_savoury_n_s\n",
      "- raisins_dried\n",
      "- ice_tea_on_black_tea_basis\n",
      "- kaki\n",
      "- smoothie\n",
      "- crepe_with_flour_plain\n",
      "- nuggets\n",
      "- chili_con_carne_prepared\n",
      "- veggie_burger\n",
      "- chinese_cabbage\n",
      "- hamburger\n",
      "- soup_pumpkin\n",
      "- sushi\n",
      "- chestnuts_ch\n",
      "- sauce_soya\n",
      "- balsamic_salad_dressing\n",
      "- pasta_twist\n",
      "- bolognaise_sauce\n",
      "- leek\n",
      "- fajita_bread_only\n",
      "- potato_gnocchi\n",
      "- rice_noodles_vermicelli\n",
      "- bread_whole_wheat\n",
      "- onion\n",
      "- garlic\n",
      "- hummus\n",
      "- pizza_with_vegetables_baked\n",
      "- beer\n",
      "- glucose_drink_50g\n",
      "- ratatouille\n",
      "- peanut\n",
      "- cauliflower\n",
      "- green_olives\n",
      "- bread_pita\n",
      "- pasta_wholemeal\n",
      "- sauce_pesto\n",
      "- couscous\n",
      "- sauce\n",
      "- bread_toast\n",
      "- water_with_lemon_juice\n",
      "- espresso\n",
      "- egg_scrambled\n",
      "- juice_orange\n",
      "- braided_white_loaf_ch\n",
      "- emmental_cheese_ch\n",
      "- hazelnut_chocolate_spread_nutella_ovomaltine_caotina\n",
      "- tomme_ch\n",
      "- hazelnut\n",
      "- peach\n",
      "- figs\n",
      "- mashed_potatoes_prepared_with_full_fat_milk_with_butter\n",
      "- pumpkin\n",
      "- swiss_chard\n",
      "- red_cabbage_raw\n",
      "- spinach_raw\n",
      "- chicken_curry_cream_coconut_milk_curry_spices_paste\n",
      "- crunch_muesli\n",
      "- biscuit\n",
      "- meatloaf_ch\n",
      "- fresh_cheese_n_s\n",
      "- honey\n",
      "- vegetable_mix_peas_and_carrots\n",
      "- parsley\n",
      "- brownie\n",
      "- ice_cream_n_s\n",
      "- salad_dressing\n",
      "- dried_meat_n_s\n",
      "- chicken_breast\n",
      "- mixed_salad_chopped_without_sauce\n",
      "- feta\n",
      "- praline_n_s\n",
      "- walnut\n",
      "- potato_salad\n",
      "- kolhrabi\n",
      "- alfa_sprouts\n",
      "- brussel_sprouts\n",
      "- gruyere_ch\n",
      "- bulgur\n",
      "- grapes\n",
      "- chocolate_egg_small\n",
      "- cappuccino\n",
      "- crisp_bread\n",
      "- bread_black\n",
      "- rosti_n_s\n",
      "- mango\n",
      "- muesli_dry\n",
      "- spinach\n",
      "- fish_n_s\n",
      "- risotto\n",
      "- crisps_ch\n",
      "- pork_n_s\n",
      "- pomegranate\n",
      "- sweet_corn\n",
      "- flakes\n",
      "- greek_salad\n",
      "- sesame_seeds\n",
      "- bouillon\n",
      "- baked_potato\n",
      "- fennel\n",
      "- meat_n_s\n",
      "- croutons\n",
      "- bell_pepper_red_stewed\n",
      "- nuts\n",
      "- breadcrumbs_unspiced\n",
      "- fondue\n",
      "- sauce_mushroom\n",
      "- strawberries\n",
      "- pie_plum_baked_with_cake_dough\n",
      "- potatoes_au_gratin_dauphinois_prepared\n",
      "- capers\n",
      "- bread_wholemeal_toast\n",
      "- red_radish\n",
      "- fruit_tart\n",
      "- beans_kidney\n",
      "- sauerkraut\n",
      "- mustard\n",
      "- country_fries\n",
      "- ketchup\n",
      "- pasta_linguini_parpadelle_tagliatelle\n",
      "- chicken_cut_into_stripes_only_meat\n",
      "- cookies\n",
      "- sun_dried_tomatoe\n",
      "- bread_ticino_ch\n",
      "- semi_hard_cheese\n",
      "- porridge_prepared_with_partially_skimmed_milk\n",
      "- juice\n",
      "- chocolate_milk\n",
      "- bread_fruit\n",
      "- corn\n",
      "- dates\n",
      "- pistachio\n",
      "- cream_cheese_n_s\n",
      "- bread_rye\n",
      "- witloof_chicory\n",
      "- goat_cheese_soft\n",
      "- grapefruit_pomelo\n",
      "- blue_mould_cheese\n",
      "- guacamole\n",
      "- tofu\n",
      "- cordon_bleu\n",
      "- quinoa\n",
      "- kefir_drink\n",
      "- salad_rocket\n",
      "- pizza_with_ham_with_mushrooms_baked\n",
      "- fruit_coulis\n",
      "- plums\n",
      "- pizza_with_ham_baked\n",
      "- pineapple\n",
      "- seeds_n_s\n",
      "- focaccia\n",
      "- mixed_milk_beverage\n",
      "- coleslaw_chopped_without_sauce\n",
      "- sweet_potato\n",
      "- chicken_leg\n",
      "- croissant\n",
      "- cheesecake\n",
      "- sauce_cocktail\n",
      "- croissant_with_chocolate_filling\n",
      "- pumpkin_seeds\n",
      "- artichoke\n",
      "- soft_drink_with_a_taste\n",
      "- apple_pie\n",
      "- white_bread_with_butter_eggs_and_milk\n",
      "- savoury_pastry_stick\n",
      "- tuna_in_oil_drained\n",
      "- meat_terrine_pate\n",
      "- falafel_balls\n",
      "- berries_n_s\n",
      "- latte_macchiato\n",
      "- sugar_melon_galia_honeydew_cantaloupe\n",
      "- mixed_seeds_n_s\n",
      "- oil_vinegar_salad_dressing\n",
      "- celeriac\n",
      "- chocolate_mousse\n",
      "- lemon\n",
      "- chocolate_cookies\n",
      "- birchermuesli_prepared_no_sugar_added\n",
      "- muffin\n",
      "- pine_nuts\n",
      "- french_pizza_from_alsace_baked\n",
      "- chocolate_n_s\n",
      "- grits_polenta_maize_flour\n",
      "- wine_rose\n",
      "- cola_based_drink\n",
      "- raspberries\n",
      "- roll_with_pieces_of_chocolate\n",
      "- cake_lemon\n",
      "- rice_wild\n",
      "- gluten_free_bread\n",
      "- pearl_onion\n",
      "- tzatziki\n",
      "- ham_croissant_ch\n",
      "- corn_crisps\n",
      "- lentils_green_du_puy_du_berry\n",
      "- rice_whole_grain\n",
      "- cervelat_ch\n",
      "- aperitif_with_alcohol_n_s_aperol_spritz\n",
      "- peas\n",
      "- tiramisu\n",
      "- apricots\n",
      "- lasagne_meat_prepared\n",
      "- brioche\n",
      "- vegetable_au_gratin_baked\n",
      "- basil\n",
      "- butter_spread_puree_almond\n",
      "- pie_apricot\n",
      "- rusk_wholemeal\n",
      "- pasta_in_conch_form\n",
      "- pasta_in_butterfly_form_farfalle\n",
      "- damson_plum\n",
      "- shoots_n_s\n",
      "- coconut\n",
      "- banana_cake\n",
      "- sauce_curry\n",
      "- watermelon_fresh\n",
      "- white_asparagus\n",
      "- cherries\n",
      "- nectarine\n"
     ]
    }
   ],
   "source": [
    "# Reading all classes\n",
    "category_ids = train_coco.loadCats(train_coco.getCatIds())\n",
    "category_names = [_[\"name_readable\"] for _ in category_ids]\n",
    "\n",
    "print(\"## Categories\\n-\", \"\\n- \".join(category_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id   file_name  width  height\n",
      "0      131072  131072.jpg    464     464\n",
      "1      131087  131087.jpg    464     464\n",
      "2      131088  131088.jpg    511     512\n",
      "3      131094  131094.jpg    480     480\n",
      "4      131096  131096.jpg    464     464\n",
      "...       ...         ...    ...     ...\n",
      "54387  131024  131024.jpg    512     910\n",
      "54388  131033  131033.jpg    455     455\n",
      "54389  131053  131053.jpg    391     390\n",
      "54390  131066  131066.jpg    464     464\n",
      "54391  131071  131071.jpg    464     464\n",
      "\n",
      "[54392 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "img_info = pd.DataFrame(train_coco.loadImgs(train_coco.getImgIds()))\n",
    "print(img_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_no = 1\n",
    "end_no = len(train_annotations_data['images'])\n",
    "# 모든 train 데이터의 이미지애 대하여..\n",
    "for img_no in range(start_no, end_no):\n",
    "    \n",
    "    annIds = train_coco.getAnnIds(imgIds=train_annotations_data['images'][img_no]['id'])\n",
    "    anns = train_coco.loadAnns(annIds)\n",
    "\n",
    "    # load and render the image\n",
    "    plt.imshow(plt.imread(TRAIN_IMAGE_DIRECTIORY+train_annotations_data['images'][img_no]['file_name']))\n",
    "    plt.axis('off')\n",
    "    # Render annotations on top of the image\n",
    "    train_coco.showAnns(anns)\n",
    "\n",
    "    mask = train_coco.annToMask(anns[0])\n",
    "    for i in range(len(anns)):\n",
    "        mask += train_coco.annToMask(anns[i])\n",
    "        plt.imshow(mask)\n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig(fname='masks/' + str(train_annotations_data['images'][img_no]['id']) + '.png', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # plot 초기화\n",
    "    plt.clf()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change file name\n",
    "import os\n",
    "path = \"/Users/s2hee/Desktop/deep_learning/masks\"\n",
    "file_list = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in file_list:\n",
    "    newname = name.replace(\".png\", \"\")\n",
    "    newname = newname.zfill(6)\n",
    "    newname = newname + \".png\"\n",
    "    src = os.path.join(path, name)\n",
    "    dst = os.path.join(path, newname)\n",
    "    os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic U-Net Model segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def cont_block(self, in_channels, out_channels, kernel_size=3):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout2d(), # p=0.5\n",
    "                              nn.Conv2d(in_channels=out_channels, out_channels=out_channels, \n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.BatchNorm2d(out_channels))\n",
    "        return block\n",
    "    \n",
    "    def expn_block(self, in_channels, mid_channel, out_channels, kernel_size=3):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=mid_channel,\n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout2d(),\n",
    "                              nn.Conv2d(in_channels=mid_channel, out_channels=mid_channel,\n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.BatchNorm2d(mid_channel),\n",
    "                              nn.ConvTranspose2d(in_channels=mid_channel, out_channels=out_channels,\n",
    "                                                 kernel_size=kernel_size, stride=2, padding=1, output_padding=1))\n",
    "        return block\n",
    "    \n",
    "    def final_block(self, in_channels, mid_channel, out_channels, kernel_size=3):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=mid_channel,\n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout2d(),\n",
    "                              nn.Conv2d(in_channels=mid_channel, out_channels=mid_channel,\n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.BatchNorm2d(mid_channel),\n",
    "                              nn.Conv2d(in_channels=mid_channel, out_channels=out_channels,\n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.Sigmoid()\n",
    "                              )\n",
    "        return block\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(UNet, self).__init__()\n",
    "        self.contl_1 = self.cont_block(in_channels=in_channel, out_channels=64)\n",
    "        self.contl_1_mp = nn.MaxPool2d(kernel_size=2)\n",
    "        self.contl_2 = self.cont_block(in_channels=64, out_channels=128)\n",
    "        self.contl_2_mp = nn.MaxPool2d(kernel_size=2)\n",
    "        self.contl_3 = self.cont_block(in_channels=128, out_channels=256)\n",
    "        self.contl_3_mp = nn.MaxPool2d(kernel_size=2)\n",
    "        self.contl_4 = self.cont_block(in_channels=256, out_channels=512)\n",
    "        self.contl_4_mp = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.bottleneck = nn.Sequential(nn.Conv2d(in_channels=512, out_channels=1024, \n",
    "                                                  kernel_size=3, padding=1),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.BatchNorm2d(1024),\n",
    "                                        nn.Conv2d(in_channels=1024, out_channels=1024, \n",
    "                                                  kernel_size=3, padding=1),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.BatchNorm2d(1024),\n",
    "                                        nn.ConvTranspose2d(in_channels=1024, out_channels=512,\n",
    "                                                           kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "                                        )\n",
    "        \n",
    "        self.expnl_1 = self.expn_block(1024, 512, 256)\n",
    "        self.expnl_2 = self.expn_block(512, 256, 128)\n",
    "        self.expnl_3 = self.expn_block(256, 128, 64)\n",
    "        self.final_layer = self.final_block(128, 64, out_channel)\n",
    "    \n",
    "    def crop_and_concat(self, upsampled, bypass, crop=False):\n",
    "        if crop:\n",
    "            c = (bypass.size()[2] - upsampled.size()[2]) // 2\n",
    "            bypass = F.pad(bypass, (-c, -c, -c, -c))\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        contl_1_out = self.contl_1(x) \n",
    "        contl_1_pool = self.contl_1_mp(contl_1_out)\n",
    "        contl_2_out = self.contl_2(contl_1_pool)\n",
    "        contl_2_pool = self.contl_2_mp(contl_2_out)\n",
    "        contl_3_out = self.contl_3(contl_2_pool)\n",
    "        contl_3_pool = self.contl_3_mp(contl_3_out)\n",
    "        contl_4_out = self.contl_4(contl_3_pool)\n",
    "        contl_4_pool = self.contl_4_mp(contl_4_out)\n",
    "        \n",
    "        bottleneck_out = self.bottleneck(contl_4_pool)\n",
    "        \n",
    "        \"\"\" Skip connection of the expansive sub-module \"\"\"\n",
    "        expnl_1_cat = self.crop_and_concat(bottleneck_out, contl_4_out)\n",
    "        expnl_1_out = self.expnl_1(expnl_1_cat)\n",
    "        expnl_2_cat = self.crop_and_concat(expnl_1_out, contl_3_out)\n",
    "        expnl_2_out = self.expnl_2(expnl_2_cat)\n",
    "        expnl_3_cat = self.crop_and_concat(expnl_2_out, contl_2_out)\n",
    "        expnl_3_out = self.expnl_3(expnl_3_cat)\n",
    "        final_cat = self.crop_and_concat(expnl_3_out, contl_1_out)\n",
    "        final_out = self.final_layer(final_cat)\n",
    "        \n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iou definition\n",
    "def iou(pred, target, n_classes = 2):\n",
    "    \n",
    "    iou = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    # Ignore IoU for background class (\"0\")\n",
    "    for cls in range(1, n_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).long().sum().data.cpu().item()\n",
    "        union = pred_inds.long().sum().data.cpu().item() + target_inds.long().sum().data.cpu().item() - intersection\n",
    "\n",
    "        if union == 0:\n",
    "            iou.append(float('nan'))  # If there is no ground truth, do not include in evaluation\n",
    "        else:\n",
    "            iou.append(float(intersection) / float(max(union, 1)))\n",
    "\n",
    "    return sum(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def iou_metric(y_pred, y_true, n_classes = 2):\n",
    "    miou = []\n",
    "    for i in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = (y_pred > i)\n",
    "        iou_init = iou(y_pred_, y_true, n_classes = n_classes)\n",
    "        miou.append(iou_init)\n",
    "    \n",
    "    return sum(miou)/len(miou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_U_Net_.ipynb                \u001b[34mpublic_validation_set_release_2.1\u001b[m\u001b[m\n",
      "\u001b[34mpublic_training_set_release_2.1\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory\n",
    "\n",
    "TRAIN_PATH = \"/Users/s2hee/Desktop/deep_learning/public_training_set_release_2.1\"\n",
    "VAL_PATH = \"/Users/s2hee/Desktop/deep_learning/public_validation_set_release_2.1\"\n",
    "\n",
    "train_dir = os.listdir(TRAIN_PATH)\n",
    "val_dir = os.listdir(VAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'images', 'training_annotations.json']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_path = \"/Users/s2hee/Desktop/deep_learning/public_training_set_release_2.1/images\"\n",
    "train_ids = os.listdir(train_dir_path)\n",
    "val_dir_path = \"/Users/s2hee/Desktop/deep_learning/public_validation_set_release_2.1/images\"\n",
    "val_ids = os.listdir(val_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3\n",
    "width_out = 128\n",
    "height_out = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "y_train = np.zeros((len(train_ids), height_out, width_out, 1), dtype=bool)\n",
    "x_val = np.zeros((len(val_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, id_ in enumerate(train_ids):\n",
    "    path = TRAIN_PATH + id_\n",
    "    img = cv2.imread(TRAIN_PATH+'/images/'+id_)\n",
    "    img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    x_train[i] = img\n",
    "\n",
    "    mask = np.zeros((height_out, width_out, 1), dtype=bool)\n",
    "    mask_path = \"/Users/s2hee/Desktop/deep_learning/masks/\"\n",
    "    for id_ in os.listdir(mask_path):\n",
    "        mask_ = cv2.imread(mask_path+id_, 0)\n",
    "        mask_ = cv2.resize(mask_, (height_out, width_out))\n",
    "        mask_ = np.expand_dims(mask_, axis=-1)\n",
    "        mask = np.maximum(mask, mask_)\n",
    "    y_train[i] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"x_train.npy\", x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"y_train.npy\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation 없이 학습하기\n",
    "\n",
    "class UNetDataset(Dataset):\n",
    "    def __init__(self, images_np, masks_np):\n",
    "        self.images_np = images_np\n",
    "        self.masks_np = masks_np\n",
    "    \n",
    "    def transform(self, image_np, mask_np):\n",
    "        ToPILImage = transforms.ToPILImage()\n",
    "        image = ToPILImage(image_np)\n",
    "        mask = ToPILImage(mask_np.astype(np.int32))\n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_np)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_np = self.images_np[idx]\n",
    "        mask_np = self.masks_np[idx]\n",
    "        image, mask = self.transform(image_np, mask_np)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=0)\n",
    "train_dataset = UNetDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataset = UNetDataset(x_val, y_val)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_channel=3, out_channel=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "alpha = 5\n",
    "batch_size = 16\n",
    "# nn.CrossEntropyLoss() (paper) -> BCELoss()\n",
    "criterion=nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_iou_sum = 0\n",
    "valid_iou_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_iou_list = []\n",
    "val_iou_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_iou = 0\n",
    "\n",
    "    for image, mask in train_loader:\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image.float())\n",
    "    \n",
    "        loss = criterion(outputs.float(), mask.float())\n",
    "        train_loss += loss\n",
    "\n",
    "        train_iou += iou_metric(outputs, mask)\n",
    "        rev_iou = 16 - iou_metric(outputs, mask)\n",
    "        loss += alpha * rev_iou\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        valid_iou = 0\n",
    "\n",
    "        for image_val, mask_val in valid_loader:\n",
    "            image_val = image_val.to(device)\n",
    "            mask_val = mask_val.to(device)\n",
    "            output_val = model(image_val.float())\n",
    "            valid_loss += criterion(output_val.float(), mask_val.float())\n",
    "            valid_iou += iou_metric(output_val, mask_val)\n",
    "\n",
    "    print(\"Epoch \", epoch + 1, \" Training Loss: \", train_loss/len(train_loader), \"Validation Loss: \", valid_loss/len(valid_loader))\n",
    "    print(\"Training IoU: \", train_iou/len(train_loader), \"Validation IoU: \", valid_iou/len(valid_loader))\n",
    "    train_iou_sum += train_iou/len(train_loader)\n",
    "    valid_iou_sum += valid_iou/len(valid_loader)\n",
    "\n",
    "    # visualization\n",
    "    train_loss_list.append(train_loss/len(train_loader))\n",
    "    val_loss_list.append(valid_loss/len(valid_loader))\n",
    "    train_iou_list.append(train_iou/len(train_loader))\n",
    "    val_iou_list.append(valid_iou/len(valid_loader)\n",
    ")\n",
    "    \n",
    "print(\"Training Mean IoU: {:.2f}\".format(train_iou_sum/epochs), \" Validation Mean IoU: {:.2f}\".format(valid_iou_sum/epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, IOU 값 시각화하기\n",
    "\n",
    "train_loss_data = [i.detach().cpu().numpy() for i in train_loss_list]\n",
    "val_loss_data = [i.detach().cpu().numpy() for i in val_loss_list]\n",
    "# train_iou_data = [i.detach().cpu().numpy() for i in train_iou_list]\n",
    "# val_iou_data = [i.detach().cpu().numpy() for i in val_iou_list]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_loss_data, 'b', label='train loss')\n",
    "plt.plot(val_loss_data, 'g', label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('iou')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_iou_list, 'b', label='train iou')\n",
    "plt.plot(val_iou_list, 'g', label='val iou')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제값, 예측값 시각화하기\n",
    "\n",
    "for img_, mask_ in valid_loader:\n",
    "    img = img_[1].to(device)\n",
    "    img.unsqueeze_(0)\n",
    "    mask_pred = model(img.float())\n",
    "    mask_pred = mask_pred.cpu()\n",
    "    mask_pred = (mask_pred > 0.75)\n",
    "    mask_true = mask_[1]\n",
    "\n",
    "    img = TF.to_pil_image(mask_pred.float().squeeze(0))\n",
    "    mask = TF.to_pil_image(mask_true)\n",
    "\n",
    "    img = np.array(img)\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    fig, (axis_1, axis_2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    axis_1.imshow(img.astype(np.uint8), cmap='gray')\n",
    "    axis_1.set_title('Input Image')\n",
    "    axis_2.imshow(mask.astype(np.uint8), cmap='gray')\n",
    "    axis_2.set_title('Prediction')\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation 적용하여 학습하기\n",
    "\n",
    "class UNetDataset_DAug(Dataset):\n",
    "    def __init__(self, images_np, masks_np):\n",
    "        self.images_np = images_np\n",
    "        self.masks_np = masks_np\n",
    "    \n",
    "    def transform(self, image_np, mask_np):\n",
    "        ToPILImage = transforms.ToPILImage()\n",
    "        image = ToPILImage(image_np)\n",
    "        mask = ToPILImage(mask_np.astype(np.int32))\n",
    "        \n",
    "        image = TF.pad(image, padding = 20, padding_mode = 'reflect')\n",
    "        mask = TF.pad(mask, padding = 20, padding_mode = 'reflect')\n",
    "              \n",
    "        angle = random.uniform(-10, 10)\n",
    "        width, height = image.size\n",
    "        max_dx = 0.1 * width\n",
    "        max_dy = 0.1 * height\n",
    "        translations = (np.round(random.uniform(-max_dx, max_dx)), np.round(random.uniform(-max_dy, max_dy)))\n",
    "        scale = random.uniform(0.8, 1.2)\n",
    "        shear = random.uniform(-0.5, 0.5)\n",
    "        image = TF.affine(image, angle = angle, translate = translations, scale = scale, shear = shear)\n",
    "        mask = TF.affine(mask, angle = angle, translate = translations, scale = scale, shear = shear)\n",
    "        \n",
    "\n",
    "        image = TF.center_crop(image, (128, 128))\n",
    "        mask = TF.center_crop(mask, (128, 128))\n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_np)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_np = self.images_np[idx]\n",
    "        mask_np = self.masks_np[idx]\n",
    "        image, mask = self.transform(image_np, mask_np)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=0)\n",
    "train_dataset = UNetDataset_DAug(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataset = UNetDataset_DAug(x_val, y_val)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = UNet(in_channel=3, out_channel=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25 \n",
    "alpha = 5\n",
    "batch_size = 16 \n",
    "# nn.CrossEntropyLoss() (paper) -> BCELoss()\n",
    "criterion=nn.BCELoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=1e-3)\n",
    "train_iou_sum = 0\n",
    "valid_iou_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_iou_list = []\n",
    "val_iou_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model2.train()\n",
    "    train_loss = 0\n",
    "    train_iou = 0\n",
    "\n",
    "    for image, mask in train_loader:\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(image.float())\n",
    "    \n",
    "        loss = criterion(outputs.float(), mask.float())\n",
    "        train_loss += loss\n",
    "\n",
    "        train_iou += iou_metric(outputs, mask)\n",
    "        rev_iou = 16 - iou_metric(outputs, mask)\n",
    "        loss += alpha * rev_iou\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        valid_iou = 0\n",
    "\n",
    "        for image_val, mask_val in valid_loader:\n",
    "            image_val = image_val.to(device)\n",
    "            mask_val = mask_val.to(device)\n",
    "            output_val = model2(image_val.float())\n",
    "            valid_loss += criterion(output_val.float(), mask_val.float())\n",
    "            valid_iou += iou_metric(output_val, mask_val)\n",
    "\n",
    "    print(\"Epoch \", epoch + 1, \" Training Loss: \", train_loss/len(train_loader), \"Validation Loss: \", valid_loss/len(valid_loader))\n",
    "    print(\"Training IoU: \", train_iou/len(train_loader), \"Validation IoU: \", valid_iou/len(valid_loader))\n",
    "    train_iou_sum += train_iou/len(train_loader)\n",
    "    valid_iou_sum += valid_iou/len(valid_loader)\n",
    "\n",
    "    # visualization\n",
    "    train_loss_list.append(train_loss/len(train_loader))\n",
    "    val_loss_list.append(valid_loss/len(valid_loader))\n",
    "    train_iou_list.append(train_iou/len(train_loader))\n",
    "    val_iou_list.append(valid_iou/len(valid_loader)\n",
    ")\n",
    "    \n",
    "print(\"Training Mean IoU: {:.2f}\".format(train_iou_sum/epochs), \" Validation Mean IoU: {:.2f}\".format(valid_iou_sum/epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, IOU 값 시각화\n",
    "\n",
    "train_loss_data = [i.detach().cpu().numpy() for i in train_loss_list]\n",
    "val_loss_data = [i.detach().cpu().numpy() for i in val_loss_list]\n",
    "# train_iou_data = [i.detach().cpu().numpy() for i in train_iou_list]\n",
    "# val_iou_data = [i.detach().cpu().numpy() for i in val_iou_list]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_loss_data, 'b', label='train loss')\n",
    "plt.plot(val_loss_data, 'g', label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('iou')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_iou_list, 'b', label='train iou')\n",
    "plt.plot(val_iou_list, 'g', label='val iou')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 실제값, 예측값 시각화\n",
    "\n",
    "for img_, mask_ in valid_loader:\n",
    "    img = img_[1].to(device)\n",
    "    img.unsqueeze_(0)\n",
    "    mask_pred = model2(img.float())\n",
    "    mask_pred = mask_pred.cpu()\n",
    "    mask_pred = (mask_pred > 0.75)\n",
    "    mask_true = mask_[1]\n",
    "\n",
    "    img = TF.to_pil_image(mask_pred.float().squeeze(0))\n",
    "    mask = TF.to_pil_image(mask_true)\n",
    "\n",
    "    img = np.array(img)\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    fig, (axis_1, axis_2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    axis_1.imshow(img.astype(np.uint8), cmap='gray')\n",
    "    axis_1.set_title('Input Image')\n",
    "    axis_2.imshow(mask.astype(np.uint8), cmap='gray')\n",
    "    axis_2.set_title('Prediction')\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sehee')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87b29be79caa31f39437f92f2a865b23c2f4e0693e3bfc42c7f510d610300d74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
