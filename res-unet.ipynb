{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(5,5)\n",
    " \n",
    "    def forward(self,x):\n",
    "        net = self.lin1(x)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4941, -0.3022,  0.4494, -0.7746, -0.0288], device='mps:0',\n",
      "       grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# MPS 장치에 바로 tensor를 생성합니다.\n",
    "x = torch.ones(5, device=device)\n",
    " \n",
    "# GPU 상에서 연산 진행\n",
    "y = x * 2\n",
    " \n",
    "# 또는, 다른 장치와 마찬가지로 MPS로 이동할 수도 있습니다.\n",
    "model = Net()# 어떤 모델의 객체를 생성한 뒤,\n",
    "model.to(device) # MPS 장치로 이동합니다.\n",
    " \n",
    "# 이제 모델과 텐서를 호출하면 GPU에서 연산이 이뤄집니다.\n",
    "pred = model(x)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:1.14.0.dev20221107\n",
      "MPS 장치를 지원하도록 build 되었는지: True\n",
      "MPS 장치가 사용 가능한지: True\n",
      "macOS-10.16-x86_64-i386-64bit\n"
     ]
    }
   ],
   "source": [
    "print (f\"PyTorch version:{torch.__version__}\") # 1.12.1 이상\n",
    "print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\") # True 여야 합니다.\n",
    "print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\") # True 여야 합니다.\n",
    "!python -c 'import platform;print(platform.platform())'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net with Residual unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, padding):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        \"\"\" Residual block과 channel size를 맞추기 위한 conv operation \"\"\"\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1),\n",
    "        ) \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        r = self.conv_block(inputs)\n",
    "        s = self.shortcut(inputs)\n",
    "        \n",
    "        skip = r + s\n",
    "        return skip\n",
    "\n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResUNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        \"\"\" Encoder input layer \"\"\"\n",
    "        self.contl_1 = self.input_block(in_channels=3, out_channels=64)\n",
    "        self.contl_2 = self.input_skip(in_channels=3, out_channels=64)\n",
    "        \n",
    "        \"\"\" Residual encoder block \"\"\"\n",
    "        self.resdl_1 = ResidualBlock(64, 128, 2, 1)\n",
    "        self.resdl_2 = ResidualBlock(128, 256, 2, 1)\n",
    "        #self.resdl_3 = ResidualBlock(256, 512, 2, 1)\n",
    "        \n",
    "        \"\"\" Encoder decoder skip connection \"\"\"\n",
    "        self.middle = ResidualBlock(256, 512, 2, 1)\n",
    "        \n",
    "        \"\"\" Decoder block \"\"\"\n",
    "        self.expnl_1 = nn.ConvTranspose2d(in_channels=512, out_channels=256, \n",
    "                                          kernel_size=2, stride=2, padding=0)\n",
    "        self.expnl_1_cv = ResidualBlock(256+256, 256, 1, 1)\n",
    "        self.expnl_2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, \n",
    "                                          kernel_size=2, stride=2, padding=0)\n",
    "        self.expnl_2_cv = ResidualBlock(128+128, 128, 1, 1)\n",
    "        self.expnl_3 = nn.ConvTranspose2d(in_channels=128, out_channels=64, \n",
    "                                          kernel_size=2, stride=2, padding=0)\n",
    "        self.expnl_3_cv = ResidualBlock(64+64, 64, 1, 1)\n",
    "        # self.expnl_4 = nn.ConvTranspose2d(in_channels=128, out_channels=128, \n",
    "        #                                   kernel_size=2, stride=2, padding=0)\n",
    "        # self.expnl_4_cv = ResidualBlock(128+64, 64, 1, 1)\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1),\n",
    "          nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def input_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    )\n",
    "        return block\n",
    "    \n",
    "    def input_skip(self, in_channels, out_channels):\n",
    "        skip = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        return skip                         \n",
    "    \n",
    "    def forward(self, X):\n",
    "        contl_1_out = self.contl_1(X) # 64\n",
    "        contl_2_out = self.contl_2(X) # 64\n",
    "        input_out = contl_1_out + contl_2_out\n",
    "        \n",
    "        resdl_1_out = self.resdl_1(input_out) # 128\n",
    "        resdl_2_out = self.resdl_2(resdl_1_out) # 256\n",
    "        #resdl_3_out = self.resdl_3(resdl_2_out) # 512\n",
    "        \n",
    "        middle_out = self.middle(resdl_2_out) # 512\n",
    "        \n",
    "        expnl_1_out = self.expnl_1(middle_out)\n",
    "        expnl_1_cv_out = self.expnl_1_cv(torch.cat((expnl_1_out, resdl_2_out), dim=1)) # 512\n",
    "        expnl_2_out = self.expnl_2(expnl_1_cv_out) # 256\n",
    "        expnl_2_cv_out = self.expnl_2_cv(torch.cat((expnl_2_out, resdl_1_out), dim=1))\n",
    "        expnl_3_out = self.expnl_3(expnl_2_cv_out)\n",
    "        expnl_3_cv_out = self.expnl_3_cv(torch.cat((expnl_3_out, contl_1_out), dim=1))\n",
    "        # expnl_4_out = self.expnl_4(expnl_3_cv_out)\n",
    "        # expnl_4_cv_out = self.expnl_4_cv(torch.cat((expnl_4_out, input_out), dim=1))\n",
    "        \n",
    "        out = self.output(expnl_3_cv_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iou definition\n",
    "def iou(pred, target, n_classes = 2):\n",
    "    \n",
    "    iou = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    # Ignore IoU for background class (\"0\")\n",
    "    for cls in range(1, n_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).long().sum().data.cpu().item()\n",
    "        union = pred_inds.long().sum().data.cpu().item() + target_inds.long().sum().data.cpu().item() - intersection\n",
    "\n",
    "        if union == 0:\n",
    "            iou.append(float('nan'))  # If there is no ground truth, do not include in evaluation\n",
    "        else:\n",
    "            iou.append(float(intersection) / float(max(union, 1)))\n",
    "\n",
    "    return sum(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def iou_metric(y_pred, y_true, n_classes = 2):\n",
    "    miou = []\n",
    "    for i in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = (y_pred > i)\n",
    "        iou_init = iou(y_pred_, y_true, n_classes = n_classes)\n",
    "        miou.append(iou_init)\n",
    "    \n",
    "    return sum(miou)/len(miou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_train_v2.npy')\n",
    "y_train = np.load('y_train_v2.npy')\n",
    "x_val = np.load('x_val_v2.npy')\n",
    "y_val = np.load('y_val_v2.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation 없이 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation 없이 학습하기\n",
    "\n",
    "class ResUNetDataset(Dataset):\n",
    "    def __init__(self, images_np, masks_np):\n",
    "        self.images_np = images_np\n",
    "        self.masks_np = masks_np\n",
    "    \n",
    "    def transform(self, image_np, mask_np):\n",
    "        ToPILImage = transforms.ToPILImage()\n",
    "        image = ToPILImage(image_np)\n",
    "        mask = ToPILImage(mask_np.astype(np.int32))\n",
    "        \n",
    "        # image = TF.pad(image, padding = 20, padding_mode = 'reflect')\n",
    "        # mask = TF.pad(mask, padding = 20, padding_mode = 'reflect')\n",
    "        \n",
    "        # angle = random.uniform(-10, 10)\n",
    "        # width, height = image.size\n",
    "        # max_dx = 0.1 * width\n",
    "        # max_dy = 0.1 * height\n",
    "        # translations = (np.round(random.uniform(-max_dx, max_dx)), np.round(random.uniform(-max_dy, max_dy)))\n",
    "        # scale = random.uniform(0.8, 1.2)\n",
    "        # shear = random.uniform(-0.5, 0.5)\n",
    "        # image = TF.affine(image, angle = angle, translate = translations, scale = scale, shear = shear)\n",
    "        # mask = TF.affine(mask, angle = angle, translate = translations, scale = scale, shear = shear)\n",
    "        \n",
    "        # image = TF.center_crop(image, (128, 128))\n",
    "        # mask = TF.center_crop(mask, (128, 128))\n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_np)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_np = self.images_np[idx]\n",
    "        mask_np = self.masks_np[idx]\n",
    "        image, mask = self.transform(image_np, mask_np)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ResUNetDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataset = ResUNetDataset(x_val, y_val)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResUNet(\n",
      "  (contl_1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (contl_2): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (resdl_1): ResidualBlock(\n",
      "    (conv_block): Sequential(\n",
      "      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU()\n",
      "      (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (resdl_2): ResidualBlock(\n",
      "    (conv_block): Sequential(\n",
      "      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU()\n",
      "      (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (middle): ResidualBlock(\n",
      "    (conv_block): Sequential(\n",
      "      (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU()\n",
      "      (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (expnl_1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (expnl_1_cv): ResidualBlock(\n",
      "    (conv_block): Sequential(\n",
      "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU()\n",
      "      (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (expnl_2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (expnl_2_cv): ResidualBlock(\n",
      "    (conv_block): Sequential(\n",
      "      (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU()\n",
      "      (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (expnl_3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (expnl_3_cv): ResidualBlock(\n",
      "    (conv_block): Sequential(\n",
      "      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU()\n",
      "      (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ResUNet(num_classes=1).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "alpha = 5\n",
    "batch_size = 16\n",
    "criterion=nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Training Loss:  tensor(0.3986, device='mps:0', grad_fn=<DivBackward0>) Validation Loss:  tensor(0.3589, device='mps:0')\n",
      "Training mAP:  0.2931713027366018 Validation mAP:  0.26444336742853664\n",
      "Epoch  2  Training Loss:  tensor(0.3280, device='mps:0', grad_fn=<DivBackward0>) Validation Loss:  tensor(0.3261, device='mps:0')\n",
      "Training mAP:  0.39577634402941875 Validation mAP:  0.38265325585001264\n",
      "Epoch  3  Training Loss:  tensor(0.2962, device='mps:0', grad_fn=<DivBackward0>) Validation Loss:  tensor(0.3032, device='mps:0')\n",
      "Training mAP:  0.4488584187419191 Validation mAP:  0.5141132155705209\n",
      "Epoch  4  Training Loss:  tensor(0.2831, device='mps:0', grad_fn=<DivBackward0>) Validation Loss:  tensor(0.2779, device='mps:0')\n",
      "Training mAP:  0.4706799996580027 Validation mAP:  0.506414446742763\n",
      "Epoch  5  Training Loss:  tensor(0.2718, device='mps:0', grad_fn=<DivBackward0>) Validation Loss:  tensor(0.2954, device='mps:0')\n",
      "Training mAP:  0.4904988307129076 Validation mAP:  0.4264127029695788\n",
      "Epoch  6  Training Loss:  tensor(9.7682, device='mps:0', grad_fn=<DivBackward0>) Validation Loss:  tensor(29.4746, device='mps:0')\n",
      "Training mAP:  0.348566629350549 Validation mAP:  0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/s2hee/Desktop/deep_learning/res-unet.ipynb 셀 20\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/res-unet.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/res-unet.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m image, mask \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/res-unet.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/res-unet.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/res-unet.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_iou_list = []\n",
    "val_iou_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_iou = 0\n",
    "\n",
    "    i = 1\n",
    "    for image, mask in train_loader:\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image.float())\n",
    "    \n",
    "        loss = criterion(outputs.float(), mask.float())\n",
    "        train_loss += loss\n",
    "\n",
    "        train_iou += iou_metric(outputs, mask)\n",
    "        #print('[[ train',i,']] loss:',loss,'/ mAP:',iou_metric(outputs, mask))\n",
    "        #i += 1\n",
    "        rev_iou = 16 - iou_metric(outputs, mask)\n",
    "        loss += alpha * rev_iou\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        valid_iou = 0\n",
    "\n",
    "        i = 1\n",
    "        for image_val, mask_val in valid_loader:\n",
    "            image_val = image_val.to(device)\n",
    "            mask_val = mask_val.to(device)\n",
    "            output_val = model(image_val.float())\n",
    "            valid_loss += criterion(output_val.float(), mask_val.float())\n",
    "            valid_iou += iou_metric(output_val, mask_val)\n",
    "            #print('[[ train',i,']] loss:',criterion(output_val.float(), mask_val.float()),'/ mAP:',iou_metric(output_val, mask_val))\n",
    "            # i += 1\n",
    "\n",
    "    #print('------------------------------------------------------------------')\n",
    "    print(\"Epoch \", epoch + 1, \" Training Loss: \", train_loss/len(train_loader), \"Validation Loss: \", valid_loss/len(valid_loader))\n",
    "    print(\"Training mAP: \", train_iou/len(train_loader), \"Validation mAP: \", valid_iou/len(valid_loader))\n",
    "    \n",
    "    # visualization\n",
    "    train_loss_list.append(train_loss/len(train_loader))\n",
    "    val_loss_list.append(valid_loss/len(valid_loader))\n",
    "    train_iou_list.append(train_iou/len(train_loader))\n",
    "    val_iou_list.append(valid_iou/len(valid_loader)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, IOU 값 시각화하기\n",
    "\n",
    "train_loss_data = [i.detach().cpu().numpy() for i in train_loss_list]\n",
    "val_loss_data = [i.detach().cpu().numpy() for i in val_loss_list]\n",
    "# train_iou_data = [i.detach().cpu().numpy() for i in train_iou_list]\n",
    "# val_iou_data = [i.detach().cpu().numpy() for i in val_iou_list]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_loss_data, 'b', label='train loss')\n",
    "plt.plot(val_loss_data, 'g', label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('iou')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_iou_list, 'b', label='train iou')\n",
    "plt.plot(val_iou_list, 'g', label='val iou')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제값, 예측값 시각화하기\n",
    "\n",
    "for img_, mask_ in valid_loader:\n",
    "    img = img_[1].to(device)\n",
    "    img.unsqueeze_(0)\n",
    "    mask_pred = model(img.float())\n",
    "    mask_pred = mask_pred.cpu()\n",
    "    mask_pred = (mask_pred > 0.75)\n",
    "    mask_true = mask_[1]\n",
    "\n",
    "    img = TF.to_pil_image(mask_pred.float().squeeze(0))\n",
    "    mask = TF.to_pil_image(mask_true)\n",
    "\n",
    "    img = np.array(img)\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    fig, (axis_1, axis_2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    axis_1.imshow(img.astype(np.uint8), cmap='gray')\n",
    "    axis_1.set_title('Input Image')\n",
    "    axis_2.imshow(mask.astype(np.uint8), cmap='gray')\n",
    "    axis_2.set_title('Prediction')\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation 적용하여 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation 적용하여 학습하기\n",
    "\n",
    "class ResUNetDataset_DAug(Dataset):\n",
    "    def __init__(self, images_np, masks_np):\n",
    "        self.images_np = images_np\n",
    "        self.masks_np = masks_np\n",
    "    \n",
    "    def transform(self, image_np, mask_np):\n",
    "        ToPILImage = transforms.ToPILImage()\n",
    "        image = ToPILImage(image_np)\n",
    "        mask = ToPILImage(mask_np.astype(np.int32))\n",
    "        \n",
    "        image = TF.pad(image, padding = 20, padding_mode = 'reflect')\n",
    "        mask = TF.pad(mask, padding = 20, padding_mode = 'reflect')\n",
    "        \n",
    "        angle = random.uniform(-10, 10)\n",
    "        width, height = image.size\n",
    "        max_dx = 0.1 * width\n",
    "        max_dy = 0.1 * height\n",
    "        translations = (np.round(random.uniform(-max_dx, max_dx)), np.round(random.uniform(-max_dy, max_dy)))\n",
    "        scale = random.uniform(0.8, 1.2)\n",
    "        shear = random.uniform(-0.5, 0.5)\n",
    "        image = TF.affine(image, angle = angle, translate = translations, scale = scale, shear = shear)\n",
    "        mask = TF.affine(mask, angle = angle, translate = translations, scale = scale, shear = shear)\n",
    "        \n",
    "        image = TF.center_crop(image, (128, 128))\n",
    "        mask = TF.center_crop(mask, (128, 128))\n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_np)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_np = self.images_np[idx]\n",
    "        mask_np = self.masks_np[idx]\n",
    "        image, mask = self.transform(image_np, mask_np)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_train_v2.npy')\n",
    "y_train = np.load('y_train_v2.npy')\n",
    "x_val = np.load('x_val_v2.npy')\n",
    "y_val = np.load('y_val_v2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ResUNetDataset_DAug(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataset = ResUNetDataset_DAug(x_val, y_val)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ResUNet(num_classes=1).to(device)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25 \n",
    "alpha = 5\n",
    "batch_size = 16 \n",
    "criterion=nn.BCELoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_iou_list = []\n",
    "val_iou_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model2.train()\n",
    "    train_loss = 0\n",
    "    train_iou = 0\n",
    "\n",
    "    for image, mask in train_loader:\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(image.float())\n",
    "    \n",
    "        loss = criterion(outputs.float(), mask.float())\n",
    "        train_loss += loss\n",
    "\n",
    "        train_iou += iou_metric(outputs, mask)\n",
    "        rev_iou = 16 - iou_metric(outputs, mask)\n",
    "        loss += alpha * rev_iou\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        valid_iou = 0\n",
    "\n",
    "        for image_val, mask_val in valid_loader:\n",
    "            image_val = image_val.to(device)\n",
    "            mask_val = mask_val.to(device)\n",
    "            output_val = model2(image_val.float())\n",
    "            valid_loss += criterion(output_val.float(), mask_val.float())\n",
    "            valid_iou += iou_metric(output_val, mask_val)\n",
    "\n",
    "    print(\"Epoch \", epoch + 1, \" Training Loss: \", train_loss/len(train_loader), \"Validation Loss: \", valid_loss/len(valid_loader))\n",
    "    print(\"Training mAP: \", train_iou/len(train_loader), \"Validation mAP: \", valid_iou/len(valid_loader))\n",
    "\n",
    "    # visualization\n",
    "    train_loss_list.append(train_loss/len(train_loader))\n",
    "    val_loss_list.append(valid_loss/len(valid_loader))\n",
    "    train_iou_list.append(train_iou/len(train_loader))\n",
    "    val_iou_list.append(valid_iou/len(valid_loader)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, IOU 값 시각화\n",
    "\n",
    "train_loss_data = [i.detach().cpu().numpy() for i in train_loss_list]\n",
    "val_loss_data = [i.detach().cpu().numpy() for i in val_loss_list]\n",
    "# train_iou_data = [i.detach().cpu().numpy() for i in train_iou_list]\n",
    "# val_iou_data = [i.detach().cpu().numpy() for i in val_iou_list]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_loss_data, 'b', label='train loss')\n",
    "plt.plot(val_loss_data, 'g', label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('iou')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_iou_list, 'b', label='train iou')\n",
    "plt.plot(val_iou_list, 'g', label='val iou')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 실제값, 예측값 시각화\n",
    "\n",
    "for img_, mask_ in valid_loader:\n",
    "    img = img_[1].to(device)\n",
    "    img.unsqueeze_(0)\n",
    "    mask_pred = model2(img.float())\n",
    "    mask_pred = mask_pred.cpu()\n",
    "    mask_pred = (mask_pred > 0.75)\n",
    "    mask_true = mask_[1]\n",
    "\n",
    "    img = TF.to_pil_image(mask_pred.float().squeeze(0))\n",
    "    mask = TF.to_pil_image(mask_true)\n",
    "\n",
    "    img = np.array(img)\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    fig, (axis_1, axis_2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    axis_1.imshow(img.astype(np.uint8), cmap='gray')\n",
    "    axis_1.set_title('Input Image')\n",
    "    axis_2.imshow(mask.astype(np.uint8), cmap='gray')\n",
    "    axis_2.set_title('Prediction')\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sehee')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87b29be79caa31f39437f92f2a865b23c2f4e0693e3bfc42c7f510d610300d74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
